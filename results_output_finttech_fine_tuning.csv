flat_docs,hnsw_docs,mmr_docs,answer,query
,"page_content='Techniques for fine-tuning models
In this section, we’ll discuss two fine-tuning methods: the traditional full fine-tuning approach and
advanced techniques such as PEFT, which integrates optimizations to attain comparable results to full
fine-tuning but with higher efficiency and reduced memory and computational expenses.
Full fine-tuning
Full fine-tuning refers to the approach where all parameters/weights of a pretrained model are adjusted
using a task-specific dataset. It’s a straightforward method and is generally effective, but it might
require a considerable amount of data to avoid overfitting and compute, especially for large models.
The challenges with generic full fine-tuning methods include updating all the model parameters of
the LLMs for every downstream task. Here are some more issues to consider:
• High compute and memory requirements: Full fine-tuning can increase the cost of compute
exorbitantly, result in large memory requirements, and also result in having to update billions or
trillions of parameters in the state-of-the-art models, which could become unwieldy and inefficient.
• Catastrophic forgetting: Full fine-tuning is prone to forgetting old information once it’s fine-
tuned on new information.
• Multiple copies of the LLM: Fine-tuning requires building a full copy of the LLM for every task,
such as sentiment analysis, machine translation, and question-answering tasks, thus increasing
storage requirements. LLMs can be gigabytes in size sometimes and building multiple copies
of them for different downstream tasks may require a lot of storage space.
To tackle these challenges and make this process more efficient, a new fine-tuning technique has
emerged called PEFT that trains a small set of parameters, which might be a subset of the existing
model parameters or a set of newly added parameters, to achieve similar or better performance to the
traditional fine-tuning methods under different scenarios. By doing this, it provides almost similar
results with a lower cost in terms of compute and fewer parameter updates.
In the next section, we will discuss different types of PEFT techniques and the trade-offs between them.
58 Fine-Tuning – Building Domain-Specific LLM Applications
PEFT
PEFT addresses the challenges with full fine-tuning by training a smaller set of parameters. In this
section, we will discuss various techniques on how such efficiency can be achieved by training a smaller
set of parameters. These parameters could either be a subset of the current model’s parameters or a new
set of added parameters. These techniques vary in terms of parameter efficiency, memory efficiency,
and training speed, though model quality and any potential extra inference costs are also distinguishing
factors among these methods. PEFT techniques can be broadly classified into three categories:
• Selective
• Additive
• Reparameterization
The following figure shows 30 PEFT methods that were discussed in 40 research papers published' metadata={'pk': 458312173011974295}
---
page_content='RAG techniques, both of which we will discuss in later chapters. As highlighted previously, fine-tuning
tailors LLMs for specific tasks or domains. In LLMs, weights refer to the parameters of the neural
network, which are learned during the model’s training process and are used to calculate the output
based on input data, allowing the model to make predictions and generate text. Essentially, fine-tuning
improves a pretrained model by refining these parameters with data specific to a task.
Now, let’s consider the benefits of fine-tuning:
• Reduced hallucinations: Fine-tuning on trusted data reduces a model’s tendency to generate
incorrect or fabricated outputs.
• Better task performance: Since the model is tailored to your specific requirements, it can result
in better responses that are required for your domain-specific use case. For instance, BioGPT,
fine-tuned from GPT models using biomedical datasets, delivered enhanced answers to medical
queries compared to non-fine-tuned GPT models.
• Cost-efficiency: Although there are initial upfront costs when it comes to fine-tuning, once
the model has been fine-tuned, you don’t need to provide as many few-shot samples to the
prompt, leading to shorter prompts and lower costs. We will discuss the few-shot prompting
technique further in Chapter 5.
• Improved latency: Smaller prompts also mean lower latency requests as fewer resources are
needed by the LLM to process your API call.
• Consistent results: Fine-tuning an LLM with a domain-specific dataset enhances the consistency
and accuracy of its responses within that domain. For example, training a general language
model with a dataset of medical research papers not only enhances its response accuracy but
also ensures consistent output in that field across multiple queries. For instance, when the
model is asked to “Describe the typical symptoms of Type 2 Diabetes,” a fine-tuned model
might accurately and consistently respond, “Typical symptoms of Type 2 Diabetes include
increased thirst, frequent urination, hunger, fatigue, and blurred vision.” This specialized training
ensures the model provides more reliable information for medical inquiries, maintaining this
consistency across similar queries.
In this section, we explored the “What” and “Why” of fine tuning. Now let's understand some real-
world use cases where fine-tuning can add value to your AI application.
54 Fine-Tuning – Building Domain-Specific LLM Applications
Fine-tuning applications
Fine-tuning can be applied to a wide range of natural language processing tasks, including the following:
• Text classification: This involves classifying text into predefined categories by examining its
content or context. For example, in sentiment analysis of customer reviews, we can classify
text as positive, negative, or neutral.
• Token classification: This involves labeling words in a piece of text, often to spot names or' metadata={'pk': 458312173011974292}
---
page_content='language tasks. We will learn more about SuperGLUE in the upcoming sections of this chapter:
Figure 3.12 – Relationship between SuperGLUE Score and Model Parameters
Figure 3.12 shows the relationship between SuperGLUE Score and Model Parameters for different fine-
tuning techniques. As scale increases, prompt tuning matches model tuning, despite tuning 25,000 times
fewer parameters.
64 Fine-Tuning – Building Domain-Specific LLM Applications
The following GitHub repository from Google Research provides a code implementation of this
experiment for prompt tuning: https://github.com/google-research/prompt-tuning.
In terms of the downsides of prompt tuning, interpreting soft prompts can be challenging as these
tokens are not fixed hard prompts and do not represent natural language. To understand the nearest
meaning, you must convert the embeddings back into tokens and determine the top-k closest neighbors
by measuring the cosine similarity. This is because the closest neighbors form a semantic group with
semantic similarities.
Reparameterization
Regular full fine-tuning, which involves retraining all parameters in a language model, is not feasible
as the model size grows. This can become computationally very expensive. Hence, researchers have
identified a new method called reparameterization, a technique that’s used in fine-tuning to reduce
the number of trainable parameters in a model while maintaining its effectiveness. These methods
use low-rank transformation to reparameterize the weights, thus reducing the number of trainable
parameters while still allowing the method to work with high-dimensional matrices such as the
pre-trained parameters of the networks. Let’s explore a very popular reparameterization method
called Low-Rank Adaptation (LoRa).
LoRA
To enhance the efficiency of fine-tuning, LoRA utilizes a method where weight updates are depicted
using two compact matrices via low-rank decomposition. This approach entails locking the pre-trained
model weights and introducing trainable rank decomposition matrices into each layer of the Transformer
architecture. Low-rank decomposition, often simply referred to as low-rank approximation, is a
mathematical method that’s used to approximate a given matrix with the product of two lower-rank
matrices. The primary goal of this technique is to capture the most important information contained
in the original matrix while using fewer parameters or dimensions. The experimental results indicated
that LoRa can reduce the number of trainable parameters by more than 96%.
The following figure shows the difference between regular fine-tuning and LoRA. As you can see, the
weight update, W_delta, that was identified during backpropagation in full fine-tuning is decomposed
into two low-rank matrices in LoRA. W_a and W_b provide the same information as the original
W_delta but in a more efficient representation:
Techniques for fine-tuning models 65
Figure 3.13 – Comparing regular full fine-tuning and LoRA' metadata={'pk': 458312173011974299}
---
page_content='factors among these methods. PEFT techniques can be broadly classified into three categories:
• Selective
• Additive
• Reparameterization
The following figure shows 30 PEFT methods that were discussed in 40 research papers published
between February 2019 and February 2023:
Figure 3.6 – PEFT methods that were discussed in research papers published between 2019 and 2023
This diagram was taken from a survey published in the paper Scale Down to Scale Up: A Guide to
Parameter-Efficient Tuning.
We will dive into each of these categories in this section but only cover the most important PEFT
techniques that have shown promising results.
Techniques for fine-tuning models 59
Additive
The core concept of additive methods involves fine-tuning a model by adding extra parameters or
layers, exclusively training these new parameters, and keeping the original model weights frozen.
Although these techniques introduce new parameters to the network, they effectively reduce training
times and increase memory efficiency by decreasing the size of gradients and the optimizer states. This
is the most widely explored category of PEFT methods. A prominent method under this category is
prompt tuning with soft prompts.
Prompt tuning with soft prompts
This type of tuning involves freezing the model weights and updating the prompt parameters instead
of model parameters like in model fine-tuning. When you freeze the weights of a model, you prevent
them from being updated during training. These weights remain the same throughout the fine-
tuning process. It is a very compute and energy-efficient technique compared to traditional fine-
tuning. Prompt tuning should not be confused with prompt engineering, which we will discuss in
Chapter 5. To understand prompt tuning better, we need to understand the concept of soft prompts
and embedding space.
Soft prompts and embedding space
An embedding vector space is a high-dimensional space where words, phrases, or other types of data
are represented as vectors such that semantically similar items are located close to each other in the
space. In the context of natural language processing, these embeddings capture semantic meanings
and relationships between words or sentences, allowing for operations that can infer similarities,
analogies, and other linguistic patterns.
Figure 3.7 – Soft prompts versus hard prompts
60 Fine-Tuning – Building Domain-Specific LLM Applications
The above figure depicts a 3D embedding vector space along the X, Y, and Z axes. Representing natural
language through tokens is considered to be challenging because each token is associated with a specific
location in the embedding vector space. Hence, they are also referred to as hard prompts. On the
other hand, soft prompts are not confined to fixed, discrete words in natural language and can assume
any value in the multi-dimensional embedding vector space. In the following figure, words such as' metadata={'pk': 458312173011974296}
---
page_content='The model is trained to predict the next word in a sentence, given the previous words. The result of
pre-training is a model that has learned a general understanding of language and can generate coherent
text. However, it lacks specificity and the ability to generate targeted or domain-specific content.
The foundation for creating more advanced models lies in utilizing pristine and smart training data.
The following figure shows the datasets that are used for pretraining OpenAI’s GPT-3 models. These
datasets underwent data preparation to remove duplicates and ensure diversity and lack of bias before
being used for pre-training:
Figure 3.3 – Datasets used for pretraining OpenAI’s GPT-3 models
For instance, Llama models, by Meta, were developed using the following publicly available datasets,
after thorough data purification and deduplication:
56 Fine-Tuning – Building Domain-Specific LLM Applications
Figure 3.4 – Llama model pre-training data
This training dataset consisted of 1.4 trillion tokens after tokenization. We discussed the concept of
tokens briefly in Chapter 2 and will discuss it in more detail in Chapter 5.
Fine-tuning process
Fine-tuning is the second phase of training a language model and occurs after pre-training. During
this phase, the model is trained on a more specific dataset that is carefully curated and customized for
a particular task or domain. This dataset is often referred to as the “fine-tuning dataset.” The model is
fed with data from the fine-tuning dataset, following which it predicts the next tokens and evaluates
its predictions against the actual, or “ground truth,” values. In this process, it tries to minimize the
loss. By doing this repetitively, the LLM becomes fine-tuned to the downstream task:
Figure 3.5 – The process of fine-tuning
The preceding diagram depicts a language model’s journey from pre-training to fine-tuning. Initially, it’s
trained on a broad dataset sourced from diverse internet texts capturing a variety of language constructs,
topics, and styles. Subsequently, it’s refined using a targeted, high-quality dataset with domain-specific
Techniques for fine-tuning models 57
prompts and completions. Ultimately, the data quality of this fine-tuning dataset dictates the model’s
output precision. Finally, the fine-tuned model interacts with a user through queries and responses,
catering to a particular downstream task. As discussed earlier, these downstream tasks could include
text classification, token classification, question-answering, summarization, translation, and more.
So far, we’ve explored the overarching concept of fine-tuning, weighing its advantages and limitations.
Now, let’s delve into some basic and advanced fine-tuning techniques.
Techniques for fine-tuning models
In this section, we’ll discuss two fine-tuning methods: the traditional full fine-tuning approach and
advanced techniques such as PEFT, which integrates optimizations to attain comparable results to full' metadata={'pk': 458312173011974294}","page_content='Techniques for fine-tuning models
In this section, we’ll discuss two fine-tuning methods: the traditional full fine-tuning approach and
advanced techniques such as PEFT, which integrates optimizations to attain comparable results to full
fine-tuning but with higher efficiency and reduced memory and computational expenses.
Full fine-tuning
Full fine-tuning refers to the approach where all parameters/weights of a pretrained model are adjusted
using a task-specific dataset. It’s a straightforward method and is generally effective, but it might
require a considerable amount of data to avoid overfitting and compute, especially for large models.
The challenges with generic full fine-tuning methods include updating all the model parameters of
the LLMs for every downstream task. Here are some more issues to consider:
• High compute and memory requirements: Full fine-tuning can increase the cost of compute
exorbitantly, result in large memory requirements, and also result in having to update billions or
trillions of parameters in the state-of-the-art models, which could become unwieldy and inefficient.
• Catastrophic forgetting: Full fine-tuning is prone to forgetting old information once it’s fine-
tuned on new information.
• Multiple copies of the LLM: Fine-tuning requires building a full copy of the LLM for every task,
such as sentiment analysis, machine translation, and question-answering tasks, thus increasing
storage requirements. LLMs can be gigabytes in size sometimes and building multiple copies
of them for different downstream tasks may require a lot of storage space.
To tackle these challenges and make this process more efficient, a new fine-tuning technique has
emerged called PEFT that trains a small set of parameters, which might be a subset of the existing
model parameters or a set of newly added parameters, to achieve similar or better performance to the
traditional fine-tuning methods under different scenarios. By doing this, it provides almost similar
results with a lower cost in terms of compute and fewer parameter updates.
In the next section, we will discuss different types of PEFT techniques and the trade-offs between them.
58 Fine-Tuning – Building Domain-Specific LLM Applications
PEFT
PEFT addresses the challenges with full fine-tuning by training a smaller set of parameters. In this
section, we will discuss various techniques on how such efficiency can be achieved by training a smaller
set of parameters. These parameters could either be a subset of the current model’s parameters or a new
set of added parameters. These techniques vary in terms of parameter efficiency, memory efficiency,
and training speed, though model quality and any potential extra inference costs are also distinguishing
factors among these methods. PEFT techniques can be broadly classified into three categories:
• Selective
• Additive
• Reparameterization
The following figure shows 30 PEFT methods that were discussed in 40 research papers published' metadata={'pk': 458312173011767860}
---
page_content='• How to evaluate fine-tuned model performance
• Real-life examples of fine-tuning success – InstructGPT
52 Fine-Tuning – Building Domain-Specific LLM Applications
Figure 3.1 – AI not fine-tuned for social interactions
What is fine-tuning and why does it matter?
Issues inherent in general LLMs such as GPT-3 include their tendency to produce outputs that are
false, toxic content, or negative sentiments. This is attributed to the training of LLMs, which focuses
on predicting subsequent words from vast internet text, rather than securely accomplishing the user’s
intended language task. In essence, these models lack alignment with their users’ objectives.
Let’s look at three cases that I found in the first half of 2023 that demonstrate ChatGPT’s
hallucination problems.
Case 1 – an American law professor was falsely accused of being a sexual offender by ChatGPT, with
the generated response referencing a non-existent Washington News report. If this misinformation had
gone unnoticed, it could have had severe and irreparable consequences for the professor’s reputation
(source: https://www.firstpost.com/world/chatgpt-makes-up-a-sexual-
harassment-scandal-names-real-professor-as-accused-12418552.html).
Case 2 – a lawyer used ChatGPT in court and cited fake cases. A lawyer used ChatGPT to help with
an airline lawsuit. The AI suggested fake cases, which the lawyer unknowingly presented in court. This
mistake led a judge to consider sanctions and has drawn attention to AI “hallucinations” in legal settings
(source: https://www.forbes.com/sites/mollybohannon/2023/06/08/lawyer-
used-chatgpt-in-court-and-cited-fake-cases-a-judge-is-considering-
sanctions/?sh=2f13a6c77c7f).
Case 3 – ChatGPT can fabricate information. According to ChatGPT, The New York Times first
reported on “artificial intelligence” on July 10, 1956, in an article titled Machines Will Be Capable
of Learning, Solving Problems, Scientists Predict. However, it’s crucial to note that while the 1956
Dartmouth College conference mentioned in the response was real, the article itself did not exist;
ChatGPT generated this information. This highlights how ChatGPT can not only provide incorrect
What is fine-tuning and why does it matter? 53
information but also fabricate details, including names, dates, medical explanations, book plots, internet
addresses, and even historical events that never occurred (source: https://www.nytimes.
com/2023/05/01/business/ai-chatbots-hallucination.html).
Note
The aforementioned hallucination problems occurred in the first half of 2023. Since then,
OpenAI has put strict measures and hallucination mitigation systems in place.
To curb hallucinations, fine-tuning is one of the potential options besides prompt engineering and
RAG techniques, both of which we will discuss in later chapters. As highlighted previously, fine-tuning
tailors LLMs for specific tasks or domains. In LLMs, weights refer to the parameters of the neural' metadata={'pk': 458312173011767856}
---
page_content='location in the embedding vector space. Hence, they are also referred to as hard prompts. On the
other hand, soft prompts are not confined to fixed, discrete words in natural language and can assume
any value in the multi-dimensional embedding vector space. In the following figure, words such as
“jump,” “fox,” and others are hard prompts, whereas the unlabeled black-colored token is a soft prompt.
Prompt tuning process
In prompt tuning, soft prompts, also known as virtual tokens, are concatenated with the prompts;
it’s left to a supervised training process to determine the optimal values. As shown in the following
figure, these trainable soft tokens are prepended to an embedding vector representation – in this case,
“The student learns science:”
Figure 3.8 – Soft prompt concatenation
The following figure provides a more detailed representation of the process. Vectors are attached to
the beginning of each embedded input vector and fed into the model, the prediction is compared
to the target to calculate a loss, and the error is backpropagated to calculate gradients, but only the
new learnable vectors are updated, keeping the core model frozen. In other words, we are searching
the embedding space for the best representation of the prompt that the LLMs should accept. Even
though we can’t easily understand soft prompts learned this way, they can help us figure out how to
do a task using the labeled dataset, doing the same job as text prompts written by hand but without
being limited to specific words or phrases:
Figure 3.9 – Prompt tuning process (detailed)
Techniques for fine-tuning models 61
Next, we’ll compare three methods: model tuning (full fine-tuning), prompt tuning, and prompt design
(prompt engineering). As shown in Figure 3.10, research conducted by Google shows the difference
between model tuning, prompt tuning, and prompt design (Guiding Frozen Language Models with
Learned Soft Prompts, QUINTA-FEIRA, FEVEREIRO 10, 2022, posted by Brian Lester, AI Resident,
and Noah Constant, Senior Staff Software Engineer, Google Research).
Model tuning (full fine-tuning):
• This method starts with a pre-trained model that is then further trained (or “tuned”) on a
specific task using additional input data. The model becomes more specialized in this process.
• This method represents “strong task performance” as the model gets more aligned with the
particular task.
Prompt tuning:
• Instead of tuning the entire model, only the prompt or input to the model is adjusted. The main
model remains “frozen” or unchanged.
• This introduces the concept of “tunable soft prompts,” which can be adjusted to get desired
outputs from the model.
• This method combines the general capabilities of the pre-trained model with a more task-
specific approach, leading to “efficient multitask serving.”
Prompt design (prompt engineering):
• The focus is on designing a very specific input or prompt to guide the pre-trained model to
produce the desired output.' metadata={'pk': 458312173011767862}
---
page_content='language tasks. We will learn more about SuperGLUE in the upcoming sections of this chapter:
Figure 3.12 – Relationship between SuperGLUE Score and Model Parameters
Figure 3.12 shows the relationship between SuperGLUE Score and Model Parameters for different fine-
tuning techniques. As scale increases, prompt tuning matches model tuning, despite tuning 25,000 times
fewer parameters.
64 Fine-Tuning – Building Domain-Specific LLM Applications
The following GitHub repository from Google Research provides a code implementation of this
experiment for prompt tuning: https://github.com/google-research/prompt-tuning.
In terms of the downsides of prompt tuning, interpreting soft prompts can be challenging as these
tokens are not fixed hard prompts and do not represent natural language. To understand the nearest
meaning, you must convert the embeddings back into tokens and determine the top-k closest neighbors
by measuring the cosine similarity. This is because the closest neighbors form a semantic group with
semantic similarities.
Reparameterization
Regular full fine-tuning, which involves retraining all parameters in a language model, is not feasible
as the model size grows. This can become computationally very expensive. Hence, researchers have
identified a new method called reparameterization, a technique that’s used in fine-tuning to reduce
the number of trainable parameters in a model while maintaining its effectiveness. These methods
use low-rank transformation to reparameterize the weights, thus reducing the number of trainable
parameters while still allowing the method to work with high-dimensional matrices such as the
pre-trained parameters of the networks. Let’s explore a very popular reparameterization method
called Low-Rank Adaptation (LoRa).
LoRA
To enhance the efficiency of fine-tuning, LoRA utilizes a method where weight updates are depicted
using two compact matrices via low-rank decomposition. This approach entails locking the pre-trained
model weights and introducing trainable rank decomposition matrices into each layer of the Transformer
architecture. Low-rank decomposition, often simply referred to as low-rank approximation, is a
mathematical method that’s used to approximate a given matrix with the product of two lower-rank
matrices. The primary goal of this technique is to capture the most important information contained
in the original matrix while using fewer parameters or dimensions. The experimental results indicated
that LoRa can reduce the number of trainable parameters by more than 96%.
The following figure shows the difference between regular fine-tuning and LoRA. As you can see, the
weight update, W_delta, that was identified during backpropagation in full fine-tuning is decomposed
into two low-rank matrices in LoRA. W_a and W_b provide the same information as the original
W_delta but in a more efficient representation:
Techniques for fine-tuning models 65
Figure 3.13 – Comparing regular full fine-tuning and LoRA' metadata={'pk': 458312173011767864}
---
page_content='RLHF. This method leverages human insights to further tailor model behaviors and outputs, aligning
them more closely with human values and expectations. Let’s delve into how RLHF works and its
significance in the fine-tuning landscape.
RLHF – aligning models with human values
Fine-tuning can be beneficial for achieving specific tasks, thus enhancing accuracy and improving
model adaptability, but models can sometimes exhibit undesirable behavior. They might result in
harmful language, displaying aggression, or even sharing detailed guidance on dangerous subjects
such as weapons or explosive manufacturing. Such behaviors could be detrimental to society. This
stems from the fact that models are trained on extensive internet data, which can contain malicious
content. Both the pre-training phase and the fine-tuning process might yield outcomes that are
counterproductive, hazardous, or misleading. Hence, it’s imperative to make sure that models resonate
with human ethics and values. An added refinement step should integrate the three fundamental human
principles: helpfulness, harmlessness, and honesty (HHH). RLHF is a method of training machine
learning models, particularly in the context of reinforcement learning (RL), that uses feedback from
humans. To understand RLHF, we must understand the concept of RL:
• RL: This is a type of machine learning where an agent learns to make decisions by taking
actions in an environment to maximize some notion of cumulative reward. The agent interacts
with the environment, receives feedback in the form of rewards or penalties, and adjusts its
RLHF – aligning models with human values 67
actions accordingly. For example, a chess-playing AI improves its strategies by earning points
for winning moves and losing points for blunders.
RLHF is a type of RL where the traditional reward signal, which usually comes from the
environment, is replaced or augmented with feedback from humans. Initially, a model is trained
to imitate human behavior. Then, instead of relying solely on environmental rewards, humans
provide feedback by comparing different action sequences or trajectories. This human feedback
is used to train a reward model, which then guides the agent’s learning process, helping it
improve its decisions and actions in the environment. The core components of RLHF are the
reward model and the RL algorithm.
• Reward model: In the context of RL, a reward model is a model that provides a numerical
reward signal to an agent based on the actions it takes in a given state. Instead of manually
designing a reward function, which can be challenging and error-prone, a reward model is
learned from data, often incorporating human feedback.
• Human feedback: As shown in the following figure, the outputs from LLM models are ranked
by humans with a scoring system and then fed into the reward model. After the learning process,
the reward model is used to teach the agent what is helpful, harmless, and honest by showing' metadata={'pk': 458312173011767866}","Fine-tuning models involves adjusting a pre-trained model to better perform specific tasks or adapt to particular domains. There are several techniques for fine-tuning models, each with its own advantages and trade-offs. Here are the main techniques discussed in the context:

1. **Full Fine-Tuning**:
   - **Description**: This traditional approach involves adjusting all the parameters/weights of a pre-trained model using a task-specific dataset.
   - **Advantages**: It is straightforward and generally effective, allowing the model to become highly specialized for a particular task.
   - **Challenges**:
     - High compute and memory requirements due to the need to update potentially billions of parameters.
     - Risk of catastrophic forgetting, where the model loses previously learned information when fine-tuned on new data.
     - Requires multiple copies of the model for different tasks, increasing storage needs.

2. **Parameter-Efficient Fine-Tuning (PEFT)**:
   - **Description**: PEFT techniques train a smaller set of parameters, which might be a subset of the existing model parameters or a new set of added parameters. This approach aims to achieve similar or better performance compared to full fine-tuning but with higher efficiency.
   - **Categories**:
     - **Selective**: Focuses on fine-tuning only a selected subset of the model's parameters.
     - **Additive**: Involves adding new parameters to the model and fine-tuning them.
     - **Reparameterization**: Uses techniques like low-rank adaptation to reduce the number of trainable parameters while maintaining effectiveness.
   - **Advantages**: Reduces computational and memory costs, and minimizes the risk of catastrophic forgetting.

3. **Prompt Tuning**:
   - **Description**: Instead of tuning the entire model, only the input prompt is adjusted. The main model remains unchanged, or ""frozen.""
   - **Advantages**: Combines the general capabilities of the pre-trained model with a task-specific approach, leading to efficient multitask serving.
   - **Challenges**: Interpreting soft prompts can be difficult, as they do not represent natural language and require conversion back into tokens for understanding.

4. **Reparameterization Techniques (e.g., LoRA)**:
   - **Description**: Techniques like Low-Rank Adaptation (LoRA) use low-rank decomposition to reparameterize the weights, reducing the number of trainable parameters.
   - **Advantages**: Significantly reduces the number of trainable parameters (by more than 96% in some cases) while maintaining model performance.

These techniques provide various ways to fine-tune models efficiently, balancing the trade-offs between computational cost, memory usage, and model performance.",Expain in details the techniques for fine tuning models?
